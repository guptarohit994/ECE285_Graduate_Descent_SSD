{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision as tv\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import pickle as pkl\n",
    "import random\n",
    "import tarfile\n",
    "import collections\n",
    "import math\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from opts import *\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, resume=None, momentum=0.9):\n",
    "    \n",
    "    #initialize config\n",
    "    cfg = voc\n",
    "    dataset = VOCDetection(root=dataset_root, image_sets=[('2012', 'train')],\n",
    "                            transform=SSDAugmentation(cfg['min_dim'],\n",
    "                             MEANS))\n",
    "    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "    net = ssd_net\n",
    "    \n",
    "    if device:\n",
    "        net = torch.nn.DataParallel(ssd_net)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        net = net.to(device)\n",
    "        \n",
    "    if resume:\n",
    "        print('Resuming training, loading previous training at ',resume)\n",
    "        ssd_net.load_weights(resume)\n",
    "    else:\n",
    "        vgg_weights = torch.load(basenet)\n",
    "        print('Loading base network...')\n",
    "        ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "        print('Initializing weights...')\n",
    "        ssd_net.extras.apply(weights_init)\n",
    "        ssd_net.loc.apply(weights_init)\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr, momentum,weight_decay)\n",
    "    \n",
    "    criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                             False, torch.cuda.is_available())\n",
    "\n",
    "    \n",
    "    net.train()\n",
    "    name = 'train'\n",
    "    \n",
    "    # loss counters\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    epoch = 0\n",
    "    print('Loading the dataset...')\n",
    "\n",
    "    epoch_size = len(dataset) // batch_size\n",
    "    print('Training SSD on: ',name)\n",
    "\n",
    "    step_index = 0\n",
    "\n",
    "\n",
    "    train_data_loader = data.DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True,\\\n",
    "                               collate_fn=detection_collate,pin_memory=True)\n",
    "    \n",
    "    print(\"Number of images in the training set = \" + str(len(dataset)))\n",
    "    print(\"Number of images in a mini-batch = \"+str(batch_size))\n",
    "    print(\"Number of mini-batches = \" + str(len(train_data_loader)))\n",
    "    \n",
    "    \n",
    "     # create batch iterator\n",
    "    batch_iterator = iter(train_data_loader)\n",
    "    print(\"STARTING - ITERATIONS\")\n",
    "    \n",
    "    # Stats for pickle and plotting\n",
    "    l_loss = []\n",
    "    c_loss = []\n",
    "    itr = []\n",
    "    \n",
    "    for iteration in range(0, 150):\n",
    "        \n",
    "        if iteration != 0 and (iteration % epoch_size == 0):\n",
    "            # reset epoch loss counters\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "            epoch += 1\n",
    "\n",
    "        if iteration in cfg['lr_steps']:\n",
    "            step_index += 1\n",
    "            lr_dec = lr * (gamma ** (step_index))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_dec\n",
    "\n",
    "            \n",
    "        ## load train data\n",
    "        try:\n",
    "            images, targets = next(batch_iterator)\n",
    "        except StopIteration:\n",
    "            batch_iterator = iter(train_data_loader)\n",
    "            images, targets = next(batch_iterator)\n",
    "\n",
    "\n",
    "        \n",
    "        if device:\n",
    "            images = images.cuda()\n",
    "            targets = [ann.cuda() for ann in targets]\n",
    "        else:\n",
    "            images = images\n",
    "            targets = [ann for ann in targets]\n",
    "        \n",
    "        # forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.data.item()\n",
    "        conf_loss += loss_c.data.item()\n",
    "\n",
    "        l_loss.append(loss_l.data.item())\n",
    "        c_loss.append(loss_c.data.item())\n",
    "        itr.append(iteration)\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data.item()), end=' ')\n",
    "            currentDT = datetime.datetime.now()\n",
    "            print (currentDT.strftime(\"%H:%M:%S %p\"))\n",
    "            print(\"\\n\")\n",
    "\n",
    "        \n",
    "        if iteration != 0 and iteration % 10 == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            iter_name = math.ceil(iteration/100)*100\n",
    "            torch.save(ssd_net.state_dict(), 'weights/ssd_VOC_' +str(momentum)+ '_' + repr(iter_name) + '.pth')\n",
    "            with open('stats_momentum_' +str(momentum)+'.pkl','wb') as f:\n",
    "                pkl.dump([l_loss, c_loss, itr], f)\n",
    "                \n",
    "\n",
    "    torch.save(ssd_net.state_dict(),\n",
    "               'weights/' + data_set+ '_momentum_' + str(momentum) + '.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Initialize pointers\r\n",
      "basenet = 'weights/vgg16_reducedfc.pth'\r\n",
      "data_set = 'VOC'\r\n",
      "dataset_root = voc_root = '//datasets/ee285f-public/PascalVOC2012/'\r\n",
      "save_folder = 'trained_weights/'\r\n",
      "trained_model = 'ssd_pretrained.pth'\r\n",
      "eval_save_folder = 'eval/'\r\n",
      "devkit_path = 'devkit_path/'\r\n",
      "output_dir = \"out/\"\r\n",
      "\r\n",
      "#Run related metaparameters\r\n",
      "\r\n",
      "batch_size = 32\r\n",
      "resume = None\r\n",
      "\r\n",
      "#Optimization metaparameters\r\n",
      "lr = 1e-3\r\n",
      "momentum = 0.9\r\n",
      "weight_decay = 5e-4\r\n",
      "gamma = 0.1\r\n",
      "    \r\n",
      "confidence_threshold = 0.01\r\n",
      "top_k = 5\r\n",
      "cleanup = True\r\n",
      "\r\n",
      "YEAR = '2012'\r\n",
      "dataset_mean = (104, 117, 123)\r\n",
      "set_type = 'train'\r\n"
     ]
    }
   ],
   "source": [
    "!cat opts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please run the below cells twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Number of images in the training set = 5717\n",
      "Number of images in a mini-batch = 32\n",
      "Number of mini-batches = 179\n",
      "STARTING - ITERATIONS\n",
      "timer: 56.4053 sec.\n",
      "iter 0 || Loss: 25.0151 || 06:51:25 AM\n",
      "\n",
      "\n",
      "timer: 3.3949 sec.\n",
      "iter 10 || Loss: 15.6071 || 06:52:36 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 10\n",
      "timer: 2.7956 sec.\n",
      "iter 20 || Loss: 15.3809 || 06:54:59 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 20\n",
      "timer: 3.8933 sec.\n",
      "iter 30 || Loss: 15.2311 || 06:57:36 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 30\n",
      "timer: 2.4992 sec.\n",
      "iter 40 || Loss: 14.7566 || 07:00:01 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 40\n",
      "timer: 1.9987 sec.\n",
      "iter 50 || Loss: 14.2782 || 07:02:39 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 50\n",
      "timer: 2.8956 sec.\n",
      "iter 60 || Loss: 13.8230 || 07:04:54 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 60\n"
     ]
    }
   ],
   "source": [
    "momentum = 0.1\n",
    "train(device, \"weights/ssd_VOC_0.1_200.pth\", momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Number of images in the training set = 5717\n",
      "Number of images in a mini-batch = 32\n",
      "Number of mini-batches = 179\n",
      "STARTING - ITERATIONS\n",
      "timer: 59.9189 sec.\n",
      "iter 0 || Loss: 25.7631 || 07:43:35 AM\n",
      "\n",
      "\n",
      "timer: 3.8020 sec.\n",
      "iter 10 || Loss: nan || 07:44:47 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 10\n",
      "timer: 2.0916 sec.\n",
      "iter 20 || Loss: nan || 07:47:01 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 20\n",
      "timer: 3.8022 sec.\n",
      "iter 30 || Loss: nan || 07:50:00 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 30\n",
      "timer: 1.7055 sec.\n",
      "iter 40 || Loss: nan || 07:52:05 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 40\n",
      "timer: 1.4999 sec.\n",
      "iter 50 || Loss: nan || 07:55:12 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 50\n",
      "timer: 2.0949 sec.\n",
      "iter 60 || Loss: nan || 07:57:25 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 60\n",
      "timer: 1.9010 sec.\n",
      "iter 70 || Loss: nan || 08:00:25 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 70\n",
      "timer: 1.4930 sec.\n",
      "iter 80 || Loss: nan || 08:02:38 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 80\n",
      "timer: 2.2078 sec.\n",
      "iter 90 || Loss: nan || 08:05:43 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 90\n",
      "timer: 1.5975 sec.\n",
      "iter 100 || Loss: nan || 08:07:50 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 100\n",
      "timer: 2.9049 sec.\n",
      "iter 110 || Loss: nan || 08:10:52 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 110\n",
      "timer: 2.6969 sec.\n",
      "iter 120 || Loss: nan || 08:13:12 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 120\n",
      "timer: 1.8020 sec.\n",
      "iter 130 || Loss: nan || 08:16:23 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 130\n",
      "timer: 2.0975 sec.\n",
      "iter 140 || Loss: nan || 08:18:30 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 140\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trained_weights/VOC.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-05dd6a2c2b74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-0d41d216b38b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, resume, momentum)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     torch.save(ssd_net.state_dict(),\n\u001b[0;32m--> 135\u001b[0;31m                save_folder + data_set + '.pth') \n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_weights/VOC.pth'"
     ]
    }
   ],
   "source": [
    "momentum = 10\n",
    "train(device, resume, momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Number of images in the training set = 5717\n",
      "Number of images in a mini-batch = 32\n",
      "Number of mini-batches = 179\n",
      "STARTING - ITERATIONS\n",
      "timer: 3.0926 sec.\n",
      "iter 0 || Loss: 26.1997 || 09:56:15 AM\n",
      "\n",
      "\n",
      "timer: 2.5999 sec.\n",
      "iter 10 || Loss: 16.2860 || 09:58:19 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 10\n",
      "timer: 2.5046 sec.\n",
      "iter 20 || Loss: 15.6932 || 10:01:21 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 20\n",
      "timer: 2.4899 sec.\n",
      "iter 30 || Loss: 14.9477 || 10:03:35 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 30\n",
      "timer: 3.1277 sec.\n",
      "iter 40 || Loss: 14.7577 || 10:06:33 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 40\n",
      "timer: 1.6084 sec.\n",
      "iter 50 || Loss: 14.4329 || 10:08:39 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 50\n",
      "timer: 3.9029 sec.\n",
      "iter 60 || Loss: 14.3015 || 10:11:43 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 60\n",
      "timer: 2.6942 sec.\n",
      "iter 70 || Loss: 13.8160 || 10:13:48 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 70\n",
      "timer: 3.4991 sec.\n",
      "iter 80 || Loss: 13.8718 || 10:16:44 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 80\n",
      "timer: 2.1974 sec.\n",
      "iter 90 || Loss: 12.7966 || 10:19:03 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 90\n",
      "timer: 3.0905 sec.\n",
      "iter 100 || Loss: 12.2790 || 10:21:46 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 100\n",
      "timer: 1.6985 sec.\n",
      "iter 110 || Loss: 11.6941 || 10:24:14 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 110\n",
      "timer: 2.1974 sec.\n",
      "iter 120 || Loss: 10.7511 || 10:26:54 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 120\n",
      "timer: 1.6013 sec.\n",
      "iter 130 || Loss: 10.2433 || 10:29:37 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 130\n",
      "timer: 3.0910 sec.\n",
      "iter 140 || Loss: 9.5378 || 10:32:07 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 140\n"
     ]
    }
   ],
   "source": [
    "momentum = 0.01\n",
    "train(device, resume, momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Number of images in the training set = 5717\n",
      "Number of images in a mini-batch = 32\n",
      "Number of mini-batches = 179\n",
      "STARTING - ITERATIONS\n",
      "timer: 2.9991 sec.\n",
      "iter 0 || Loss: 26.9479 || 10:36:11 AM\n",
      "\n",
      "\n",
      "timer: 2.1992 sec.\n",
      "iter 10 || Loss: 410151948047.0588 || 10:38:21 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 10\n",
      "timer: 2.0081 sec.\n",
      "iter 20 || Loss: nan || 10:41:16 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 20\n",
      "timer: 1.9995 sec.\n",
      "iter 30 || Loss: nan || 10:43:37 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 30\n",
      "timer: 4.0017 sec.\n",
      "iter 40 || Loss: nan || 10:46:22 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 40\n",
      "timer: 1.8991 sec.\n",
      "iter 50 || Loss: nan || 10:48:47 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 50\n",
      "timer: 2.0058 sec.\n",
      "iter 60 || Loss: nan || 10:51:34 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 60\n",
      "timer: 1.9007 sec.\n",
      "iter 70 || Loss: nan || 10:53:52 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 70\n",
      "timer: 2.0991 sec.\n",
      "iter 80 || Loss: nan || 10:56:59 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 80\n",
      "timer: 1.6039 sec.\n",
      "iter 90 || Loss: nan || 10:59:12 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-096cead557ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-625ca5c20eee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, resume, momentum)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mloss_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_l\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/home-01/38/738/ksugumar/FINAL_REPO/ECE285_Graduate_Descent_SSD/layers/modules/multibox_loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             match(self.threshold, truths, defaults, self.variance, labels,\n\u001b[0;32m---> 73\u001b[0;31m                   loc_t, conf_t, idx)\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mloc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/home-01/38/738/ksugumar/FINAL_REPO/ECE285_Graduate_Descent_SSD/layers/box_utils.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(threshold, truths, priors, variances, labels, loc_t, conf_t, idx)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# ensure every gt matches with its prior of max overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_prior_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mbest_truth_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_prior_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtruths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_truth_idx\u001b[0m\u001b[0;34m]\u001b[0m          \u001b[0;31m# Shape: [num_priors,4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_truth_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m         \u001b[0;31m# Shape: [num_priors]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "momentum = 1.5\n",
    "train(device, resume, momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
