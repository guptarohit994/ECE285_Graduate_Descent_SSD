{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision as tv\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import pickle as pkl\n",
    "import random\n",
    "import tarfile\n",
    "import collections\n",
    "import math\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from opts import *\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, resume=None):\n",
    "    \n",
    "    #initialize config\n",
    "    cfg = voc\n",
    "    dataset = VOCDetection(root=dataset_root, image_sets=[('2012', 'train')],\n",
    "                            transform=SSDAugmentation(cfg['min_dim'],\n",
    "                             MEANS))\n",
    "    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "        \n",
    "    if resume:\n",
    "        print('Resuming training, loading previous training at ',resume)\n",
    "        ssd_net.load_weights(resume) \n",
    "    else:\n",
    "        vgg_weights = torch.load(basenet)\n",
    "        print('Loading base network...')\n",
    "        ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "        print('Initializing weights...')\n",
    "        ssd_net.extras.apply(weights_init)\n",
    "        ssd_net.loc.apply(weights_init)\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "        \n",
    "    net = ssd_net\n",
    "    if device:\n",
    "        net = torch.nn.DataParallel(ssd_net)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        net = net.to(device)\n",
    "        \n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "    criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                             False, torch.cuda.is_available())\n",
    "\n",
    "    \n",
    "    net.train()\n",
    "    name = 'train'\n",
    "    \n",
    "    # loss counters\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    epoch = 0\n",
    "    print('Loading the dataset...')\n",
    "\n",
    "    epoch_size = len(dataset) // batch_size\n",
    "    print('Training SSD on: ',name)\n",
    "\n",
    "    step_index = 0\n",
    "\n",
    "    train_data_loader = data.DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True,\\\n",
    "                               collate_fn=detection_collate,pin_memory=True)\n",
    "    \n",
    "    print(\"Number of images in the training set = \" + str(len(dataset)))\n",
    "    print(\"Number of images in a mini-batch = \"+str(batch_size))\n",
    "    print(\"Number of mini-batches = \" + str(len(train_data_loader)))\n",
    "    \n",
    "    \n",
    "     # create batch iterator\n",
    "    batch_iterator = iter(train_data_loader)\n",
    "    print(\"STARTING - ITERATIONS\")\n",
    "    \n",
    "    # Stats for pickle and plotting\n",
    "    l_loss = []\n",
    "    c_loss = []\n",
    "    itr = []\n",
    "    \n",
    "    for iteration in range(0, 10000):\n",
    "        \n",
    "        if iteration != 0 and (iteration % epoch_size == 0):\n",
    "            # reset epoch loss counters\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "            epoch += 1\n",
    "\n",
    "        if iteration in cfg['lr_steps']:\n",
    "            step_index += 1\n",
    "            lr_dec = lr * (gamma ** (step_index))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_dec\n",
    "\n",
    "            \n",
    "        ## load train data\n",
    "        try:\n",
    "            images, targets = next(batch_iterator)\n",
    "        except StopIteration:\n",
    "            batch_iterator = iter(train_data_loader)\n",
    "            images, targets = next(batch_iterator)\n",
    "\n",
    "\n",
    "        \n",
    "        if device:\n",
    "            images = images.cuda()\n",
    "            targets = [ann.cuda() for ann in targets]\n",
    "        else:\n",
    "            images = images\n",
    "            targets = [ann for ann in targets]\n",
    "        \n",
    "        # forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.data.item()\n",
    "        conf_loss += loss_c.data.item()\n",
    "\n",
    "        l_loss.append(loss_l.data.item())\n",
    "        c_loss.append(loss_c.data.item())\n",
    "        itr.append(iteration)\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data.item()), end=' ')\n",
    "            currentDT = datetime.datetime.now()\n",
    "            print (currentDT.strftime(\"%H:%M:%S %p\"))\n",
    "            print(\"\\n\")\n",
    "\n",
    "        \n",
    "        if iteration != 0 and iteration % 10 == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            iter_name = math.ceil(iteration/100)*100\n",
    "            torch.save(ssd_net.state_dict(), 'weights/ssd_VOC_RMSProp_' +repr(iter_name) + '.pth')\n",
    "            with open('stats_RMSProp.pkl','wb') as f:\n",
    "                pkl.dump([l_loss, c_loss, itr], f)\n",
    "                \n",
    "\n",
    "    torch.save(ssd_net.state_dict(),\n",
    "               save_folder + data_set + '.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Initialize pointers\r\n",
      "basenet = 'weights/vgg16_reducedfc.pth'\r\n",
      "data_set = 'VOC'\r\n",
      "dataset_root = voc_root = '//datasets/ee285f-public/PascalVOC2012/'\r\n",
      "save_folder = 'trained_weights/'\r\n",
      "trained_model = 'ssd_pretrained.pth'\r\n",
      "eval_save_folder = 'eval/'\r\n",
      "devkit_path = 'devkit_path/'\r\n",
      "output_dir = \"out/\"\r\n",
      "\r\n",
      "#Run related metaparameters\r\n",
      "\r\n",
      "batch_size = 32\r\n",
      "resume = None\r\n",
      "\r\n",
      "#Optimization metaparameters\r\n",
      "lr = 1e-3\r\n",
      "momentum = 0.9\r\n",
      "weight_decay = 5e-4\r\n",
      "gamma = 0.1\r\n",
      "    \r\n",
      "confidence_threshold = 0.01\r\n",
      "top_k = 5\r\n",
      "cleanup = True\r\n",
      "\r\n",
      "YEAR = '2012'\r\n",
      "dataset_mean = (104, 117, 123)\r\n",
      "set_type = 'train'\r\n"
     ]
    }
   ],
   "source": [
    "!cat opts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please run the below cell twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Number of images in the training set = 5717\n",
      "Number of images in a mini-batch = 32\n",
      "Number of mini-batches = 179\n",
      "STARTING - ITERATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:107: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 40.2981 sec.\n",
      "iter 0 || Loss: 26.2113 || 23:58:33 PM\n",
      "\n",
      "\n",
      "timer: 1.8801 sec.\n",
      "iter 10 || Loss: 14.6567 || 00:00:18 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 10\n",
      "timer: 1.7036 sec.\n",
      "iter 20 || Loss: 9.9408 || 00:03:46 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 20\n",
      "timer: 1.5873 sec.\n",
      "iter 30 || Loss: 9.1238 || 00:06:10 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 30\n",
      "timer: 0.7111 sec.\n",
      "iter 40 || Loss: 8.2475 || 00:09:31 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 40\n",
      "timer: 1.9855 sec.\n",
      "iter 50 || Loss: 7.7212 || 00:12:00 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 50\n",
      "timer: 1.4101 sec.\n",
      "iter 60 || Loss: 7.6209 || 00:15:11 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 60\n",
      "timer: 1.8928 sec.\n",
      "iter 70 || Loss: 8.0540 || 00:17:49 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 70\n",
      "timer: 1.1025 sec.\n",
      "iter 80 || Loss: 7.7291 || 00:21:02 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 80\n",
      "timer: 1.6981 sec.\n",
      "iter 90 || Loss: 8.1830 || 00:23:50 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 90\n",
      "timer: 1.3117 sec.\n",
      "iter 100 || Loss: 7.6514 || 00:26:42 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 100\n",
      "timer: 0.9958 sec.\n",
      "iter 110 || Loss: 7.3139 || 00:29:37 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 110\n",
      "timer: 2.6116 sec.\n",
      "iter 120 || Loss: 7.8322 || 00:32:30 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 120\n",
      "timer: 0.9069 sec.\n",
      "iter 130 || Loss: 7.5016 || 00:35:17 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 130\n",
      "timer: 1.0746 sec.\n",
      "iter 140 || Loss: 7.4031 || 00:38:09 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 140\n",
      "timer: 1.7963 sec.\n",
      "iter 150 || Loss: 7.4362 || 00:41:05 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 150\n",
      "timer: 3.7122 sec.\n",
      "iter 160 || Loss: 7.3211 || 00:44:09 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 160\n",
      "timer: 0.8901 sec.\n",
      "iter 170 || Loss: 8.2779 || 00:46:51 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 170\n",
      "timer: 1.2148 sec.\n",
      "iter 180 || Loss: 7.6724 || 00:50:02 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 180\n",
      "timer: 1.6963 sec.\n",
      "iter 190 || Loss: 7.4628 || 00:52:25 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 190\n",
      "timer: 1.5987 sec.\n",
      "iter 200 || Loss: 7.2702 || 00:55:56 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 200\n",
      "timer: 2.8134 sec.\n",
      "iter 210 || Loss: 7.6464 || 00:58:14 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 210\n",
      "timer: 1.4069 sec.\n",
      "iter 220 || Loss: 7.1828 || 01:01:37 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 220\n",
      "timer: 1.4786 sec.\n",
      "iter 230 || Loss: 7.4497 || 01:04:00 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 230\n",
      "timer: 1.1046 sec.\n",
      "iter 240 || Loss: 7.4825 || 01:07:10 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 240\n",
      "timer: 1.2075 sec.\n",
      "iter 250 || Loss: 7.7094 || 01:09:39 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 250\n",
      "timer: 0.8875 sec.\n",
      "iter 260 || Loss: 7.1422 || 01:12:48 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 260\n",
      "timer: 0.9176 sec.\n",
      "iter 270 || Loss: 7.2427 || 01:15:14 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 270\n",
      "timer: 1.3073 sec.\n",
      "iter 280 || Loss: 6.8423 || 01:18:38 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 280\n",
      "timer: 3.8271 sec.\n",
      "iter 290 || Loss: 7.7085 || 01:20:58 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 290\n",
      "timer: 1.1112 sec.\n",
      "iter 300 || Loss: 7.2840 || 01:24:16 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 300\n",
      "timer: 2.2779 sec.\n",
      "iter 310 || Loss: 7.3938 || 01:26:45 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 310\n",
      "timer: 1.5902 sec.\n",
      "iter 320 || Loss: 7.3973 || 01:30:00 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 320\n",
      "timer: 1.0194 sec.\n",
      "iter 330 || Loss: 7.1296 || 01:32:21 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 330\n",
      "timer: 1.1711 sec.\n",
      "iter 340 || Loss: 7.7450 || 01:35:52 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 340\n",
      "timer: 1.3860 sec.\n",
      "iter 350 || Loss: 7.4090 || 01:38:19 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 350\n",
      "timer: 1.0993 sec.\n",
      "iter 360 || Loss: 7.0081 || 01:41:17 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 360\n",
      "timer: 1.3013 sec.\n",
      "iter 370 || Loss: 7.8084 || 01:44:43 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 370\n",
      "timer: 1.8128 sec.\n",
      "iter 380 || Loss: 7.0505 || 01:47:01 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 380\n",
      "timer: 1.0085 sec.\n",
      "iter 390 || Loss: 7.2797 || 01:50:31 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 390\n",
      "timer: 1.6079 sec.\n",
      "iter 400 || Loss: 7.1095 || 01:52:56 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 400\n",
      "timer: 2.1825 sec.\n",
      "iter 410 || Loss: 7.0046 || 01:56:16 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 410\n",
      "timer: 1.2269 sec.\n",
      "iter 420 || Loss: 6.9079 || 01:58:44 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 420\n",
      "timer: 0.8145 sec.\n",
      "iter 430 || Loss: 7.2864 || 02:02:00 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 430\n",
      "timer: 2.3756 sec.\n",
      "iter 440 || Loss: 7.3127 || 02:04:33 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 440\n",
      "timer: 1.0022 sec.\n",
      "iter 450 || Loss: 6.8939 || 02:07:51 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 450\n",
      "timer: 1.7081 sec.\n",
      "iter 460 || Loss: 7.0355 || 02:10:21 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 460\n",
      "timer: 2.1075 sec.\n",
      "iter 470 || Loss: 6.9716 || 02:13:31 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 470\n",
      "timer: 1.8019 sec.\n",
      "iter 480 || Loss: 7.1286 || 02:16:04 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 480\n",
      "timer: 1.4974 sec.\n",
      "iter 490 || Loss: 7.4030 || 02:19:05 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 490\n",
      "timer: 1.6102 sec.\n",
      "iter 500 || Loss: 7.0153 || 02:21:40 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 500\n",
      "timer: 1.3923 sec.\n",
      "iter 510 || Loss: 6.8117 || 02:24:36 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 510\n",
      "timer: 1.8833 sec.\n",
      "iter 520 || Loss: 7.2920 || 02:27:16 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 520\n",
      "timer: 1.5910 sec.\n",
      "iter 530 || Loss: 7.4774 || 02:29:58 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 530\n",
      "timer: 2.5847 sec.\n",
      "iter 540 || Loss: 6.9356 || 02:32:08 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 540\n",
      "timer: 1.9895 sec.\n",
      "iter 550 || Loss: 7.3479 || 02:35:15 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 550\n",
      "timer: 1.4924 sec.\n",
      "iter 560 || Loss: 6.9124 || 02:37:27 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 560\n",
      "timer: 2.5064 sec.\n",
      "iter 570 || Loss: 6.5066 || 02:40:36 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 570\n",
      "timer: 2.2949 sec.\n",
      "iter 580 || Loss: 6.7142 || 02:42:56 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 580\n",
      "timer: 1.7979 sec.\n",
      "iter 590 || Loss: 7.1129 || 02:46:00 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 590\n",
      "timer: 2.9958 sec.\n",
      "iter 600 || Loss: 6.8345 || 02:48:22 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 600\n",
      "timer: 3.1092 sec.\n",
      "iter 610 || Loss: 7.2873 || 02:51:18 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 610\n",
      "timer: 2.1988 sec.\n",
      "iter 620 || Loss: 6.7541 || 02:53:29 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 620\n",
      "timer: 3.9059 sec.\n",
      "iter 630 || Loss: 7.1694 || 02:56:37 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 630\n",
      "timer: 1.6013 sec.\n",
      "iter 640 || Loss: 7.2082 || 02:58:42 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 640\n",
      "timer: 2.8041 sec.\n",
      "iter 650 || Loss: 6.5331 || 03:01:36 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 650\n",
      "timer: 1.8060 sec.\n",
      "iter 660 || Loss: 6.9139 || 03:03:44 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 660\n",
      "timer: 3.6987 sec.\n",
      "iter 670 || Loss: 7.1112 || 03:06:53 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 670\n",
      "timer: 2.6038 sec.\n",
      "iter 680 || Loss: 7.1691 || 03:08:56 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 680\n",
      "timer: 1.5050 sec.\n",
      "iter 690 || Loss: 6.6721 || 03:12:00 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 690\n",
      "timer: 1.3999 sec.\n",
      "iter 700 || Loss: 6.6038 || 03:14:13 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 700\n",
      "timer: 1.6050 sec.\n",
      "iter 710 || Loss: 7.2576 || 03:17:08 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 710\n",
      "timer: 2.6105 sec.\n",
      "iter 720 || Loss: 7.2606 || 03:19:37 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 720\n",
      "timer: 1.4002 sec.\n",
      "iter 730 || Loss: 6.6401 || 03:21:47 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 730\n",
      "timer: 3.0029 sec.\n",
      "iter 740 || Loss: 7.0320 || 03:24:48 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 740\n",
      "timer: 2.5002 sec.\n",
      "iter 750 || Loss: 7.0155 || 03:27:02 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 750\n",
      "timer: 1.9110 sec.\n",
      "iter 760 || Loss: 6.7025 || 03:29:59 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 760\n",
      "timer: 1.7950 sec.\n",
      "iter 770 || Loss: 7.4247 || 03:32:15 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 770\n",
      "timer: 2.6033 sec.\n",
      "iter 780 || Loss: 6.8011 || 03:35:17 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 780\n",
      "timer: 1.6053 sec.\n",
      "iter 790 || Loss: 6.8944 || 03:37:22 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 790\n",
      "timer: 2.2011 sec.\n",
      "iter 800 || Loss: 6.8113 || 03:40:25 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 800\n",
      "timer: 2.3985 sec.\n",
      "iter 810 || Loss: 6.7038 || 03:42:38 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 810\n",
      "timer: 1.7056 sec.\n",
      "iter 820 || Loss: 6.9199 || 03:45:37 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 820\n",
      "timer: 2.1994 sec.\n",
      "iter 830 || Loss: 6.8548 || 03:47:52 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 830\n",
      "timer: 2.5904 sec.\n",
      "iter 840 || Loss: 6.7420 || 03:50:59 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 840\n",
      "timer: 3.4994 sec.\n",
      "iter 850 || Loss: 7.3299 || 03:53:08 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 850\n",
      "timer: 4.1996 sec.\n",
      "iter 860 || Loss: 6.8947 || 03:56:02 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 860\n",
      "timer: 2.3953 sec.\n",
      "iter 870 || Loss: 6.7662 || 03:58:05 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 870\n",
      "timer: 1.8216 sec.\n",
      "iter 880 || Loss: 6.8728 || 04:00:57 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 880\n",
      "timer: 1.4965 sec.\n",
      "iter 890 || Loss: 6.4071 || 04:03:06 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 890\n",
      "timer: 3.0989 sec.\n",
      "iter 900 || Loss: 6.7521 || 04:05:31 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 900\n",
      "timer: 2.3020 sec.\n",
      "iter 910 || Loss: 7.2387 || 04:07:37 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 910\n",
      "timer: 4.4859 sec.\n",
      "iter 920 || Loss: 6.7303 || 04:10:33 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 920\n",
      "timer: 2.9942 sec.\n",
      "iter 930 || Loss: 7.0442 || 04:12:41 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 930\n",
      "timer: 3.6067 sec.\n",
      "iter 940 || Loss: 6.8320 || 04:15:36 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 940\n",
      "timer: 3.0960 sec.\n",
      "iter 950 || Loss: 6.4797 || 04:17:45 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 950\n",
      "timer: 2.7900 sec.\n",
      "iter 960 || Loss: 6.5209 || 04:20:37 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 3.0939 sec.\n",
      "iter 970 || Loss: 6.5862 || 04:22:45 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 970\n",
      "timer: 3.7993 sec.\n",
      "iter 980 || Loss: 6.7210 || 04:25:43 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 980\n",
      "timer: 3.2998 sec.\n",
      "iter 990 || Loss: 6.4061 || 04:27:48 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 990\n",
      "timer: 3.5977 sec.\n",
      "iter 1000 || Loss: 6.3443 || 04:30:42 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 1000\n"
     ]
    }
   ],
   "source": [
    "train(device, resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling saved loss values after 1000 iterations for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('optimization_experiments/pickles/stats_Adam.pkl','rb') as f:\n",
    "    l_loss, c_loss, itr = pickle.load(f)\n",
    "\n",
    "l_loss = np.asarray(l_loss)\n",
    "c_loss = np.asarray(c_loss)\n",
    "itr = np.asarray(itr)\n",
    "plt.plot(itr,l_loss+c_loss)\n",
    "plt.title('Loss vs Iterations: Learning Rate=1e-3 with Adam Optimizer')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
