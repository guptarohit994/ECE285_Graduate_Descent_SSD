{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision as tv\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import pickle as pkl\n",
    "import random\n",
    "import tarfile\n",
    "import collections\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version_info[0] == 2:\n",
    "    import xml.etree.cElementTree as ET\n",
    "else:\n",
    "    import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every\n",
    "        specified step\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    lr = lr * (gamma ** (step))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def xavier(param):\n",
    "    init.xavier_uniform_(param)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def create_vis_plot(_xlabel, _ylabel, _title, _legend):\n",
    "    return viz.line(\n",
    "        X=torch.zeros((1,)).cpu(),\n",
    "        Y=torch.zeros((1, 3)).cpu(),\n",
    "        opts=dict(\n",
    "            xlabel=_xlabel,\n",
    "            ylabel=_ylabel,\n",
    "            title=_title,\n",
    "            legend=_legend\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def update_vis_plot(iteration, loc, conf, window1, window2, update_type,\n",
    "                    epoch_size=1):\n",
    "    viz.line(\n",
    "        X=torch.ones((1, 3)).cpu() * iteration,\n",
    "        Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu() / epoch_size,\n",
    "        win=window1,\n",
    "        update=update_type\n",
    "    )\n",
    "    # initialize epoch plot on first iteration\n",
    "    if iteration == 0:\n",
    "        viz.line(\n",
    "            X=torch.zeros((1, 3)).cpu(),\n",
    "            Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu(),\n",
    "            win=window2,\n",
    "            update=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pick=\"\", resume=None, start_iter=0):\n",
    "    \n",
    "    print(\"Pickle File: \"+ str(pick))\n",
    "    \n",
    "    cfg = voc\n",
    "    dataset = VOCDetection(root=dataset_root, image_sets=[('2012', 'train')],\n",
    "                            transform=SSDAugmentation(cfg['min_dim'],\n",
    "                             MEANS))\n",
    "\n",
    "    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "    net = ssd_net\n",
    "    \n",
    "    if cuda:\n",
    "        net = torch.nn.DataParallel(ssd_net)\n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "    if resume:\n",
    "        print('Resuming training, loading {}...'.format(resume))\n",
    "        ssd_net.load_weights(resume)\n",
    "    else:\n",
    "        vgg_weights = torch.load(basenet)\n",
    "        print('Loading base network...')\n",
    "        ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "    if cuda:\n",
    "         net = net.to(device)\n",
    "\n",
    "    if not resume:\n",
    "        print('Initializing weights...')\n",
    "        # initialize newly added layers' weights with xavier method\n",
    "        ssd_net.extras.apply(weights_init)\n",
    "        ssd_net.loc.apply(weights_init)\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr, momentum,weight_decay)\n",
    "    criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                             False, cuda)\n",
    "\n",
    "    net.train()\n",
    "    name = 'train'\n",
    "    \n",
    "    # loss counters\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    epoch = 0\n",
    "    print('Loading the dataset...')\n",
    "\n",
    "    epoch_size = len(dataset) // batch_size\n",
    "    print('Training SSD on: ',name)\n",
    "#    print('Using the specified args:')\n",
    "#     print(args)\n",
    "\n",
    "    step_index = 0\n",
    "\n",
    "    data_loader = data.DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True,\\\n",
    "                               collate_fn=detection_collate,pin_memory=True)\n",
    "    \n",
    "    print(\"Number of images in the training set = \" + str(len(dataset)))\n",
    "    print(\"Number of images in a mini-batch = \"+str(batch_size))\n",
    "    print(\"Number of mini-batches = \" + str(len(data_loader)))\n",
    "    \n",
    "    \n",
    "     # create batch iterator\n",
    "    batch_iterator = iter(data_loader)\n",
    "    print(\"STARTING - ITERATIONS\")\n",
    "    \n",
    "    # Stats for pickle\n",
    "    l_loss = []\n",
    "    c_loss = []\n",
    "    itr = []\n",
    "    \n",
    "    for iteration in range(start_iter, 500):\n",
    "        \n",
    "        if visdom and iteration != 0 and (iteration % epoch_size == 0):\n",
    "            update_vis_plot(epoch, loc_loss, conf_loss, epoch_plot, None,\n",
    "                             'append', epoch_size)\n",
    "            # reset epoch loss counters\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "            epoch += 1\n",
    "\n",
    "        if iteration in cfg['lr_steps']:\n",
    "            step_index += 1\n",
    "            adjust_learning_rate(optimizer, gamma, step_index)\n",
    "\n",
    "            \n",
    "        ## load train data\n",
    "        #images, targets = next(batch_iterator)\n",
    "        try:\n",
    "            images, targets = next(batch_iterator)\n",
    "        except StopIteration:\n",
    "            batch_iterator = iter(data_loader)\n",
    "            images, targets = next(batch_iterator)\n",
    "\n",
    "\n",
    "        \n",
    "        if cuda:\n",
    "            images = Variable(images.cuda())\n",
    "            targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            targets = [Variable(ann, volatile=True) for ann in targets]\n",
    "        \n",
    "        # forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.data.item()\n",
    "        conf_loss += loss_c.data.item()\n",
    "\n",
    "        l_loss.append(loss_l.data.item())\n",
    "        c_loss.append(loss_c.data.item())\n",
    "        itr.append(iteration)\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data.item()), end=' ')\n",
    "            currentDT = datetime.datetime.now()\n",
    "            print (currentDT.strftime(\"%H:%M:%S %p\"))\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        if iteration != 0 and iteration % 10 == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            iter_name = math.ceil(iteration/100)*100\n",
    "            torch.save(ssd_net.state_dict(), 'weights/ssd300_' + str(pick) + '_' +repr(iter_name) + '.pth')\n",
    "            with open('stats_'+str(pick)+'.pkl','wb') as f:\n",
    "                pkl.dump([l_loss, c_loss, itr], f)\n",
    "                \n",
    "\n",
    "    torch.save(ssd_net.state_dict(),\n",
    "               save_folder + data_set + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 'VOC'\n",
    "dataset_root = '//datasets/ee285f-public/PascalVOC2012/'\n",
    "basenet = 'weights/vgg16_reducedfc.pth'\n",
    "batch_size = 32\n",
    "resume = None\n",
    "start_iter = 0\n",
    "num_workers = 4\n",
    "cuda = True\n",
    "\n",
    "learning_rate = lr = 1e-5\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.1\n",
    "\n",
    "visdom = False\n",
    "save_folder = 'trained_weights/'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "\n",
    "# resume = 'ssd300_mAP_77.43_v2.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle File: 0.01\n",
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Number of images in the training set = 5717\n",
      "Number of images in a mini-batch = 32\n",
      "Number of mini-batches = 179\n",
      "STARTING - ITERATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 59.0077 sec.\n",
      "iter 0 || Loss: 25.4825 || 03:35:31 AM\n",
      "\n",
      "\n",
      "timer: 1.9066 sec.\n",
      "iter 10 || Loss: nan || 03:36:41 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 10\n",
      "timer: 2.3963 sec.\n",
      "iter 20 || Loss: nan || 03:39:27 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 20\n",
      "timer: 2.2903 sec.\n",
      "iter 30 || Loss: nan || 03:41:45 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 30\n",
      "timer: 3.0067 sec.\n",
      "iter 40 || Loss: nan || 03:44:31 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 40\n",
      "timer: 4.6964 sec.\n",
      "iter 50 || Loss: nan || 03:46:48 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 50\n",
      "timer: 1.7022 sec.\n",
      "iter 60 || Loss: nan || 03:49:28 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 60\n",
      "timer: 2.5008 sec.\n",
      "iter 70 || Loss: nan || 03:51:52 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 70\n",
      "timer: 2.3073 sec.\n",
      "iter 80 || Loss: nan || 03:54:31 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 80\n",
      "timer: 3.0099 sec.\n",
      "iter 90 || Loss: nan || 03:56:53 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 90\n",
      "timer: 2.1964 sec.\n",
      "iter 100 || Loss: nan || 03:59:33 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 100\n",
      "timer: 2.0050 sec.\n",
      "iter 110 || Loss: nan || 04:01:47 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 110\n",
      "timer: 4.2930 sec.\n",
      "iter 120 || Loss: nan || 04:04:40 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 120\n",
      "timer: 2.8006 sec.\n",
      "iter 130 || Loss: nan || 04:06:55 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 130\n",
      "timer: 2.9993 sec.\n",
      "iter 140 || Loss: nan || 04:09:39 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 140\n",
      "timer: 2.1995 sec.\n",
      "iter 150 || Loss: nan || 04:12:04 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 150\n",
      "timer: 2.7984 sec.\n",
      "iter 160 || Loss: nan || 04:14:41 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 160\n",
      "timer: 2.5033 sec.\n",
      "iter 170 || Loss: nan || 04:17:02 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 170\n",
      "timer: 3.3086 sec.\n",
      "iter 180 || Loss: nan || 04:19:44 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 180\n",
      "timer: 2.8060 sec.\n",
      "iter 190 || Loss: nan || 04:21:50 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 190\n",
      "timer: 3.0008 sec.\n",
      "iter 200 || Loss: nan || 04:24:44 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 200\n",
      "timer: 2.4008 sec.\n",
      "iter 210 || Loss: nan || 04:27:00 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 210\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3a3f796b0b7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpick\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-33253ca5dda9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(pick, resume, start_iter)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mloss_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_l\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/home-01/38/738/ksugumar/PROJECT_SSD/layers/modules/multibox_loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             match(self.threshold, truths, defaults, self.variance, labels,\n\u001b[0;32m---> 74\u001b[0;31m                   loc_t, conf_t, idx)\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mloc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/home-01/38/738/ksugumar/PROJECT_SSD/layers/box_utils.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(threshold, truths, priors, variances, labels, loc_t, conf_t, idx)\u001b[0m\n\u001b[1;32m     88\u001b[0m     overlaps = jaccard(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtruths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mpoint_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# (Bipartite Matching)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/home-01/38/738/ksugumar/PROJECT_SSD/layers/box_utils.py\u001b[0m in \u001b[0;36mjaccard\u001b[0;34m(box_a, box_b)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mjaccard\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mShape\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbox_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintersect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     area_a = ((box_a[:, 2]-box_a[:, 0]) *\n\u001b[1;32m     63\u001b[0m               (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
      "\u001b[0;32m/datasets/home/home-01/38/738/ksugumar/PROJECT_SSD/layers/box_utils.py\u001b[0m in \u001b[0;36mintersect\u001b[0;34m(box_a, box_b)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n\u001b[0m\u001b[1;32m     42\u001b[0m                        box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n\u001b[1;32m     43\u001b[0m     min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lr in [1e-2]:\n",
    "    train(pick=str(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle File: 0.1\n",
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Number of images in the training set = 5717\n",
      "Number of images in a mini-batch = 32\n",
      "Number of mini-batches = 179\n",
      "STARTING - ITERATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 5.3077 sec.\n",
      "iter 0 || Loss: 26.4228 || 04:31:42 AM\n",
      "\n",
      "\n",
      "timer: 2.2957 sec.\n",
      "iter 10 || Loss: nan || 04:33:49 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 10\n",
      "timer: 2.2003 sec.\n",
      "iter 20 || Loss: nan || 04:36:49 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 20\n",
      "timer: 2.8018 sec.\n",
      "iter 30 || Loss: nan || 04:38:59 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 30\n",
      "timer: 2.7966 sec.\n",
      "iter 40 || Loss: nan || 04:42:02 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 40\n",
      "timer: 2.9045 sec.\n",
      "iter 50 || Loss: nan || 04:44:11 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 50\n",
      "timer: 3.6124 sec.\n",
      "iter 60 || Loss: nan || 04:47:19 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 60\n",
      "timer: 2.1992 sec.\n",
      "iter 70 || Loss: nan || 04:49:23 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 70\n",
      "timer: 3.4068 sec.\n",
      "iter 80 || Loss: nan || 04:52:20 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 80\n",
      "timer: 2.4023 sec.\n",
      "iter 90 || Loss: nan || 04:54:32 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 90\n",
      "timer: 2.6002 sec.\n",
      "iter 100 || Loss: nan || 04:57:28 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 100\n",
      "timer: 3.7998 sec.\n",
      "iter 110 || Loss: nan || 04:59:31 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 110\n",
      "timer: 2.3064 sec.\n",
      "iter 120 || Loss: nan || 05:02:24 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 120\n",
      "timer: 1.8032 sec.\n",
      "iter 130 || Loss: nan || 05:04:32 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 130\n",
      "timer: 2.0066 sec.\n",
      "iter 140 || Loss: nan || 05:07:31 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 140\n",
      "timer: 2.2949 sec.\n",
      "iter 150 || Loss: nan || 05:09:37 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 150\n",
      "timer: 2.3041 sec.\n",
      "iter 160 || Loss: nan || 05:12:28 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 160\n",
      "timer: 1.8069 sec.\n",
      "iter 170 || Loss: nan || 05:14:35 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 170\n",
      "timer: 3.3983 sec.\n",
      "iter 180 || Loss: nan || 05:16:57 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 180\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 703.12 MiB (GPU 0; 10.92 GiB total capacity; 7.01 GiB already allocated; 619.50 MiB free; 2.73 GiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1be52cbe46be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpick\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-33253ca5dda9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(pick, resume, start_iter)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_l\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 703.12 MiB (GPU 0; 10.92 GiB total capacity; 7.01 GiB already allocated; 619.50 MiB free; 2.73 GiB cached)"
     ]
    }
   ],
   "source": [
    "for lr in [1e-1, 1e-4]:\n",
    "    train(pick=str(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle File: 0.0001\n",
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Number of images in the training set = 5717\n",
      "Number of images in a mini-batch = 32\n",
      "Number of mini-batches = 179\n",
      "STARTING - ITERATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 56.4220 sec.\n",
      "iter 0 || Loss: 26.0719 || 08:14:38 AM\n",
      "\n",
      "\n",
      "timer: 3.1056 sec.\n",
      "iter 10 || Loss: 21.0108 || 08:15:50 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 10\n",
      "timer: 3.6074 sec.\n",
      "iter 20 || Loss: 15.6640 || 08:18:40 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 20\n",
      "timer: 3.6040 sec.\n",
      "iter 30 || Loss: 15.6045 || 08:21:08 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 30\n",
      "timer: 1.6076 sec.\n",
      "iter 40 || Loss: 15.2274 || 08:23:55 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 40\n",
      "timer: 2.1958 sec.\n",
      "iter 50 || Loss: 15.0279 || 08:26:23 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 50\n",
      "timer: 1.8016 sec.\n",
      "iter 60 || Loss: 15.0481 || 08:29:06 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 60\n",
      "timer: 1.8047 sec.\n",
      "iter 70 || Loss: 14.5396 || 08:31:54 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 70\n",
      "timer: 2.0944 sec.\n",
      "iter 80 || Loss: 14.8459 || 08:34:35 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 80\n",
      "timer: 1.9997 sec.\n",
      "iter 90 || Loss: 13.8513 || 08:37:35 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 90\n",
      "timer: 2.2004 sec.\n",
      "iter 100 || Loss: 13.9908 || 08:40:07 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 100\n",
      "timer: 2.1047 sec.\n",
      "iter 110 || Loss: 12.9512 || 08:43:01 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 110\n",
      "timer: 1.7073 sec.\n",
      "iter 120 || Loss: 12.4531 || 08:45:28 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 120\n",
      "timer: 2.5915 sec.\n",
      "iter 130 || Loss: 11.8653 || 08:48:28 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 130\n",
      "timer: 2.1042 sec.\n",
      "iter 140 || Loss: 11.2084 || 08:50:57 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 140\n",
      "timer: 1.7984 sec.\n",
      "iter 150 || Loss: 10.8675 || 08:53:58 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 150\n",
      "timer: 2.6888 sec.\n",
      "iter 160 || Loss: 9.2861 || 08:56:19 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 160\n",
      "timer: 2.5975 sec.\n",
      "iter 170 || Loss: 9.1785 || 08:59:33 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 170\n",
      "timer: 2.1139 sec.\n",
      "iter 180 || Loss: 9.2115 || 09:02:11 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 180\n",
      "timer: 1.7912 sec.\n",
      "iter 190 || Loss: 9.3676 || 09:04:31 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 190\n",
      "timer: 1.6103 sec.\n",
      "iter 200 || Loss: 9.5017 || 09:07:46 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 200\n",
      "timer: 1.9025 sec.\n",
      "iter 210 || Loss: 8.6249 || 09:10:04 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 210\n",
      "timer: 2.0994 sec.\n",
      "iter 220 || Loss: 8.8643 || 09:13:25 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 220\n",
      "timer: 2.5087 sec.\n",
      "iter 230 || Loss: 8.5344 || 09:15:44 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 230\n",
      "timer: 2.2091 sec.\n",
      "iter 240 || Loss: 7.9665 || 09:19:01 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 240\n"
     ]
    }
   ],
   "source": [
    "for lr in [1e-4]:\n",
    "    train(pick=str(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
