{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from data import VOC_CLASSES as labelmap\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision as tv\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import pickle\n",
    "import random\n",
    "import tarfile\n",
    "import collections\n",
    "\n",
    "from opts import *\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer(object):\n",
    "    \"\"\"A simple timer.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.time_metrics = [0.] * 5\n",
    "\n",
    "    def tic(self):\n",
    "        # using time.time instead of time.clock because time time.clock\n",
    "        # does not normalize for multithreading\n",
    "        self.time_metrics[0] = time.time()\n",
    "\n",
    "    def toc(self, average=True):\n",
    "        self.time_metrics[1] = time.time() - self.time_metrics[0]\n",
    "        self.time_metrics[2] += self.time_metrics[1]\n",
    "        self.time_metrics[3] += 1\n",
    "        self.time_metrics[4] = self.time_metrics[2] / self.time_metrics[3]\n",
    "        if average:\n",
    "            return self.time_metrics[4]\n",
    "        else:\n",
    "            return self.time_metrics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rec(filename):\n",
    "    \"\"\" Parse a PASCAL VOC xml file \"\"\"\n",
    "    tree = ET.parse(filename)\n",
    "    objects = []\n",
    "    for obj in tree.findall('object'):\n",
    "        obj_struct = {}\n",
    "        obj_struct['name'] = obj.find('name').text\n",
    "        obj_struct['pose'] = obj.find('pose').text\n",
    "        obj_struct['truncated'] = int(obj.find('truncated').text)\n",
    "        obj_struct['difficult'] = int(obj.find('difficult').text)\n",
    "        bbox = obj.find('bndbox')\n",
    "        obj_struct['bbox'] = [int(bbox.find('xmin').text) - 1,\n",
    "                              int(bbox.find('ymin').text) - 1,\n",
    "                              int(bbox.find('xmax').text) - 1,\n",
    "                              int(bbox.find('ymax').text) - 1]\n",
    "        objects.append(obj_struct)\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_voc_results_file_template(image_set, cls):\n",
    "    filename = 'det_' + image_set + '_%s.txt' % (cls)\n",
    "    filedir = os.path.join(devkit_path, 'results')\n",
    "    if not os.path.exists(filedir):\n",
    "        os.makedirs(filedir)\n",
    "    path = os.path.join(filedir, filename)\n",
    "    return path\n",
    "\n",
    "\n",
    "def write_voc_results_file(all_boxes, dataset):\n",
    "    for cls_ind, cls in enumerate(labelmap):\n",
    "        print('Writing {:s} VOC results file'.format(cls))\n",
    "        filename = get_voc_results_file_template(set_type, cls)\n",
    "        \n",
    "        with open(filename, 'wt') as f:\n",
    "            for im_ind, index in enumerate(dataset.ids):\n",
    "                dets = all_boxes[cls_ind+1][im_ind]\n",
    "                if dets == []:\n",
    "                    continue\n",
    "\n",
    "                for k in range(dets.shape[0]):\n",
    "                    f.write('{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n'.\n",
    "                            format(index[1], dets[k, -1],\n",
    "                                   dets[k, 0] + 1, dets[k, 1] + 1,\n",
    "                                   dets[k, 2] + 1, dets[k, 3] + 1))\n",
    "\n",
    "\n",
    "def do_python_eval(output_dir='output'):\n",
    "    cachedir = os.path.join(devkit_path, 'annotations_cache')\n",
    "    aps = []\n",
    "    \n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "        \n",
    "    for i, cls in enumerate(labelmap):\n",
    "        filename = get_voc_results_file_template(set_type, cls)\n",
    "        rec, prec, ap = voc_eval(\n",
    "           filename, annopath, imgsetpath.format(set_type), cls, cachedir,\n",
    "           ovthresh=0.5)\n",
    "        aps += [ap]\n",
    "        print('AP for ',cls,': ', ap)\n",
    "        with open(os.path.join(output_dir, cls + '_pr.pkl'), 'wb') as f:\n",
    "            pickle.dump({'rec': rec, 'prec': prec, 'ap': ap}, f)\n",
    "    print('Mean Average Precision(mAP): ',np.mean(aps))\n",
    "    print('Results: ')\n",
    "    for ap in aps:\n",
    "        print(ap)\n",
    "    print(aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_ap(rec, prec):\n",
    "    \"\"\"\n",
    "    Create Metric\n",
    "    \"\"\"\n",
    "    # 11 point metric\n",
    "    ap = 0.\n",
    "    for t in np.arange(0., 1.1, 0.1):\n",
    "        if np.sum(rec >= t) == 0:\n",
    "            p = 0\n",
    "        else:\n",
    "            p = np.max(prec[rec >= t])\n",
    "        ap = ap + p / 11.\n",
    "    return ap\n",
    "\n",
    "\n",
    "def voc_eval(detpath, annopath, imagesetfile, classname, cachedir, ovthresh=0.5):\n",
    "\n",
    "    \"\"\"\n",
    "    Top level function that does the PASCAL VOC evaluation.\n",
    "    \"\"\"\n",
    "# detections: detpath\n",
    "# annotations: annopath\n",
    "# imagesetfile: each line an image's name\n",
    "# cachedir: caches the annotations\n",
    "\n",
    "    if not os.path.isdir(cachedir):\n",
    "        os.mkdir(cachedir)\n",
    "    cachefile = os.path.join(cachedir, 'annots.pkl')\n",
    "    # read list of images\n",
    "    with open(imagesetfile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    imagenames = [x.strip() for x in lines]\n",
    "    if not os.path.isfile(cachefile):\n",
    "        # load annots\n",
    "        recs = {}\n",
    "        for i, imagename in enumerate(imagenames):\n",
    "            recs[imagename] = parse_rec(annopath % (imagename))\n",
    "            if i % 100 == 0:\n",
    "                print('Reading annotation for {:d}/{:d}'.format(\n",
    "                   i + 1, len(imagenames)))\n",
    "        # save\n",
    "        print('Saving cached annotations to {:s}'.format(cachefile))\n",
    "        with open(cachefile, 'wb') as f:\n",
    "            pickle.dump(recs, f)\n",
    "    else:\n",
    "        # load\n",
    "        with open(cachefile, 'rb') as f:\n",
    "            recs = pickle.load(f)\n",
    "\n",
    "    # extract gt objects for this class\n",
    "    class_recs = {}\n",
    "    npos = 0\n",
    "    for imagename in imagenames:\n",
    "        R = [obj for obj in recs[imagename] if obj['name'] == classname]\n",
    "        bbox = np.array([x['bbox'] for x in R])\n",
    "        difficult = np.array([x['difficult'] for x in R]).astype(np.bool)\n",
    "        det = [False] * len(R)\n",
    "        npos = npos + sum(~difficult)\n",
    "        class_recs[imagename] = {'bbox': bbox,\n",
    "                                 'difficult': difficult,\n",
    "                                 'det': det}\n",
    "\n",
    "    # read dets\n",
    "    detfile = detpath.format(classname)\n",
    "    with open(detfile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    if any(lines) == 1:\n",
    "\n",
    "        splitlines = [x.strip().split(' ') for x in lines]\n",
    "        image_ids = [x[0] for x in splitlines]\n",
    "        confidence = np.array([float(x[1]) for x in splitlines])\n",
    "        BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n",
    "\n",
    "        # sort by confidence\n",
    "        sorted_ind = np.argsort(-confidence)\n",
    "        sorted_scores = np.sort(-confidence)\n",
    "        BB = BB[sorted_ind, :]\n",
    "        image_ids = [image_ids[x] for x in sorted_ind]\n",
    "\n",
    "        # go down dets and mark TPs and FPs\n",
    "        nd = len(image_ids)\n",
    "        tp = np.zeros(nd)\n",
    "        fp = np.zeros(nd)\n",
    "        for d in range(nd):\n",
    "            R = class_recs[image_ids[d]]\n",
    "            bb = BB[d, :].astype(float)\n",
    "            ovmax = -np.inf\n",
    "            BBGT = R['bbox'].astype(float)\n",
    "            if BBGT.size > 0:\n",
    "                # compute overlaps\n",
    "                # intersection\n",
    "                ixmin = np.maximum(BBGT[:, 0], bb[0])\n",
    "                iymin = np.maximum(BBGT[:, 1], bb[1])\n",
    "                ixmax = np.minimum(BBGT[:, 2], bb[2])\n",
    "                iymax = np.minimum(BBGT[:, 3], bb[3])\n",
    "                iw = np.maximum(ixmax - ixmin, 0.)\n",
    "                ih = np.maximum(iymax - iymin, 0.)\n",
    "                inters = iw * ih\n",
    "                uni = ((bb[2] - bb[0]) * (bb[3] - bb[1]) +\n",
    "                       (BBGT[:, 2] - BBGT[:, 0]) *\n",
    "                       (BBGT[:, 3] - BBGT[:, 1]) - inters)\n",
    "                overlaps = inters / uni\n",
    "                ovmax = np.max(overlaps)\n",
    "                jmax = np.argmax(overlaps)\n",
    "\n",
    "            if ovmax > ovthresh:\n",
    "                if not R['difficult'][jmax]:\n",
    "                    if not R['det'][jmax]:\n",
    "                        tp[d] = 1.\n",
    "                        R['det'][jmax] = 1\n",
    "                    else:\n",
    "                        fp[d] = 1.\n",
    "            else:\n",
    "                fp[d] = 1.\n",
    "\n",
    "        # precision recall\n",
    "        fp = np.cumsum(fp)\n",
    "        tp = np.cumsum(tp)\n",
    "        rec = tp / float(npos)\n",
    "        \n",
    "        # ground truth\n",
    "        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "        ap = voc_ap(rec, prec)\n",
    "    else:\n",
    "        rec = -1.\n",
    "        prec = -1.\n",
    "        ap = -1.\n",
    "\n",
    "    return rec, prec, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net(eval_save_folder, net, cuda, dataset, transform, top_k,\n",
    "             im_size=300, thresh=0.05):\n",
    "    num_images = len(dataset)\n",
    "    all_boxes = [[[] for _ in range(num_images)]\n",
    "                 for _ in range(len(labelmap)+1)]\n",
    "\n",
    "    # timers\n",
    "    _t = {'im_detect': Timer(), 'misc': Timer()}\n",
    "    \n",
    "    output_dir = \"out/\"\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    det_file = os.path.join(output_dir, 'detections.pkl')\n",
    "\n",
    "    for i in range(num_images):\n",
    "        im, gt, h, w = dataset.pull_item(i)\n",
    "\n",
    "        x = im.unsqueeze(0)\n",
    "        if cuda=='cuda':\n",
    "            x = x.cuda()\n",
    "        _t['im_detect'].tic()\n",
    "        detections = net(x).data\n",
    "        detect_time = _t['im_detect'].toc(average=False)\n",
    "\n",
    "        # skip j = 0, because it's the background class\n",
    "        for j in range(1, detections.size(1)):\n",
    "            dets = detections[0, j, :]\n",
    "            mask = dets[:, 0].gt(0.).expand(5, dets.size(0)).t()\n",
    "            dets = torch.masked_select(dets, mask).view(-1, 5)\n",
    "            if dets.size(0) == 0:\n",
    "                continue\n",
    "            boxes = dets[:, 1:]\n",
    "            boxes[:, 0] *= w\n",
    "            boxes[:, 2] *= w\n",
    "            boxes[:, 1] *= h\n",
    "            boxes[:, 3] *= h\n",
    "            scores = dets[:, 0].cpu().numpy()\n",
    "            cls_dets = np.hstack((boxes.cpu().numpy(),\n",
    "                                  scores[:, np.newaxis])).astype(np.float32,\n",
    "                                                                 copy=False)\n",
    "            all_boxes[j][i] = cls_dets\n",
    "\n",
    "        print('image_detect: {:d}/{:d} {:.3f}s'.format(i + 1,\n",
    "                                                    num_images, detect_time))\n",
    "\n",
    "    with open(det_file, 'wb') as f:\n",
    "        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('\\nEvaluating detections')\n",
    "    evaluate_detections(all_boxes, output_dir, dataset)\n",
    "    \n",
    "def evaluate_detections(box_list, output_dir, dataset):\n",
    "    write_voc_results_file(box_list, dataset)\n",
    "    do_python_eval(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Initialize pointers\r\n",
      "basenet = 'weights/vgg16_reducedfc.pth'\r\n",
      "data_set = 'VOC'\r\n",
      "dataset_root = voc_root = '//datasets/ee285f-public/PascalVOC2012/'\r\n",
      "save_folder = 'trained_weights/'\r\n",
      "#trained_model = 'ssd_pretrained.pth'\r\n",
      "eval_save_folder = 'eval/'\r\n",
      "devkit_path = 'devkit_path/'\r\n",
      "output_dir = \"out/\"\r\n",
      "\r\n",
      "#Run related metaparameters\r\n",
      "\r\n",
      "batch_size = 32\r\n",
      "resume = None\r\n",
      "\r\n",
      "#Optimization metaparameters\r\n",
      "lr = 1e-3\r\n",
      "momentum = 0.9\r\n",
      "weight_decay = 5e-4\r\n",
      "gamma = 0.1\r\n",
      "    \r\n",
      "confidence_threshold = 0.01\r\n",
      "top_k = 5\r\n",
      "cleanup = True\r\n",
      "\r\n",
      "YEAR = '2012'\r\n",
      "dataset_mean = (104, 117, 123)\r\n",
      "set_type = 'train'\r\n",
      "trained_model = 'weights/ssd300_mAP_77.43_v2.pth'\r\n"
     ]
    }
   ],
   "source": [
    "!cat opts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, 'v'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7631ee34c497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelmap\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m                      \u001b[0;31m# +1 for background\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_ssd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# initialize SSD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished loading model!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, 'v'."
     ]
    }
   ],
   "source": [
    "voc_root = dataset_root\n",
    "annopath = os.path.join(voc_root, 'Annotations', '%s.xml')\n",
    "imgpath = os.path.join(voc_root, 'JPEGImages', '%s.jpg')\n",
    "imgsetpath = os.path.join(voc_root, 'ImageSets',\n",
    "                          'Main', '{:s}.txt')\n",
    "YEAR = '2012'\n",
    "\n",
    "devkit_path = 'devkit_path/'\n",
    "dataset_mean = (104, 117, 123)\n",
    "set_type = 'val'\n",
    "\n",
    "num_classes = len(labelmap) + 1                      # +1 for background\n",
    "net = build_ssd('test', 300, num_classes)            # initialize SSD\n",
    "net.load_state_dict(torch.load(trained_model))\n",
    "net.eval()\n",
    "print('Finished loading model!')\n",
    "    # load data\n",
    "\n",
    "dataset = VOCDetection(voc_root, [('2012', set_type)],\n",
    "                BaseTransform(300, dataset_mean),VOCAnnotationTransform())\n",
    "\n",
    "if torch_device=='cuda':\n",
    "    net = net.cuda()\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# evaluation   \n",
    "test_net(save_folder, net, torch_device, dataset,\n",
    "            BaseTransform(net.size, dataset_mean), top_k, 300,\n",
    "             thresh=confidence_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
