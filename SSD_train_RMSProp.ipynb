{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision as tv\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import pickle as pkl\n",
    "import random\n",
    "import tarfile\n",
    "import collections\n",
    "import math\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from opts import *\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, resume=None):\n",
    "    \n",
    "    #initialize config\n",
    "    cfg = voc\n",
    "    dataset = VOCDetection(root=dataset_root, image_sets=[('2012', 'train')],\n",
    "                            transform=SSDAugmentation(cfg['min_dim'],\n",
    "                             MEANS))\n",
    "    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "    net = ssd_net\n",
    "    \n",
    "    if train_device:\n",
    "        net = torch.nn.DataParallel(ssd_net)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        net = net.to(train_device)\n",
    "        \n",
    "    if resume:\n",
    "        print('Resuming training, loading previous training at ',resume)\n",
    "        ssd_net.load_weights(resume) \n",
    "    else:\n",
    "        vgg_weights = torch.load(basenet)\n",
    "        print('Loading base network...')\n",
    "        ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "        print('Initializing weights...')\n",
    "        ssd_net.extras.apply(weights_init)\n",
    "        ssd_net.loc.apply(weights_init)\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "    criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                             False, torch.cuda.is_available())\n",
    "\n",
    "    \n",
    "    net.train()\n",
    "    name = 'train'\n",
    "    \n",
    "    # loss counters\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    epoch = 0\n",
    "    print('Loading the dataset...')\n",
    "\n",
    "    epoch_size = len(dataset) // batch_size\n",
    "    print('Training SSD on: ',name)\n",
    "\n",
    "    step_index = 0\n",
    "\n",
    "\n",
    "    train_data_loader = data.DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True,\\\n",
    "                               collate_fn=detection_collate,pin_memory=True)\n",
    "    \n",
    "    print(\"Number of images in the training set = \" + str(len(dataset)))\n",
    "    print(\"Number of images in a mini-batch = \"+str(batch_size))\n",
    "    print(\"Number of mini-batches = \" + str(len(train_data_loader)))\n",
    "    \n",
    "    \n",
    "     # create batch iterator\n",
    "    batch_iterator = iter(train_data_loader)\n",
    "    print(\"STARTING - ITERATIONS\")\n",
    "    \n",
    "    # Stats for pickle and plotting\n",
    "    l_loss = []\n",
    "    c_loss = []\n",
    "    itr = []\n",
    "    \n",
    "    for iteration in range(0, 10000):\n",
    "        \n",
    "        if iteration != 0 and (iteration % epoch_size == 0):\n",
    "            # reset epoch loss counters\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "            epoch += 1\n",
    "\n",
    "        if iteration in cfg['lr_steps']:\n",
    "            step_index += 1\n",
    "            lr_dec = lr * (gamma ** (step_index))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_dec\n",
    "\n",
    "            \n",
    "        ## load train data\n",
    "        try:\n",
    "            images, targets = next(batch_iterator)\n",
    "        except StopIteration:\n",
    "            batch_iterator = iter(train_data_loader)\n",
    "            images, targets = next(batch_iterator)\n",
    "\n",
    "\n",
    "        \n",
    "        if train_device:\n",
    "            images = images.cuda()\n",
    "            targets = [ann.cuda() for ann in targets]\n",
    "        else:\n",
    "            images = images\n",
    "            targets = [ann for ann in targets]\n",
    "        \n",
    "        # forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.data.item()\n",
    "        conf_loss += loss_c.data.item()\n",
    "\n",
    "        l_loss.append(loss_l.data.item())\n",
    "        c_loss.append(loss_c.data.item())\n",
    "        itr.append(iteration)\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data.item()), end=' ')\n",
    "            currentDT = datetime.datetime.now()\n",
    "            print (currentDT.strftime(\"%H:%M:%S %p\"))\n",
    "            print(\"\\n\")\n",
    "\n",
    "        \n",
    "        if iteration != 0 and iteration % 10 == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            iter_name = math.ceil(iteration/100)*100\n",
    "            torch.save(ssd_net.state_dict(), 'weights/ssd_VOC_RMSProp_' +repr(iter_name) + '.pth')\n",
    "            with open('stats_RMSProp.pkl','wb') as f:\n",
    "                pkl.dump([l_loss, c_loss, itr], f)\n",
    "                \n",
    "\n",
    "    torch.save(ssd_net.state_dict(),\n",
    "               save_folder + data_set + '.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Initialize pointers\r\n",
      "basenet = 'weights/vgg16_reducedfc.pth'\r\n",
      "data_set = 'VOC'\r\n",
      "dataset_root = voc_root = '//datasets/ee285f-public/PascalVOC2012/'\r\n",
      "save_folder = 'trained_weights/'\r\n",
      "trained_model = 'ssd_pretrained.pth'\r\n",
      "eval_save_folder = 'eval/'\r\n",
      "devkit_path = 'devkit_path/'\r\n",
      "output_dir = \"out/\"\r\n",
      "\r\n",
      "#Run related metaparameters\r\n",
      "\r\n",
      "batch_size = 32\r\n",
      "resume = None\r\n",
      "\r\n",
      "#Optimization metaparameters\r\n",
      "lr = 1e-3\r\n",
      "momentum = 0.9\r\n",
      "weight_decay = 5e-4\r\n",
      "gamma = 0.1\r\n",
      "    \r\n",
      "confidence_threshold = 0.01\r\n",
      "top_k = 5\r\n",
      "cleanup = True\r\n",
      "\r\n",
      "YEAR = '2012'\r\n",
      "dataset_mean = (104, 117, 123)\r\n",
      "set_type = 'train'\r\n"
     ]
    }
   ],
   "source": [
    "!cat opts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Number of images in the training set = 5717\n",
      "Number of images in a mini-batch = 32\n",
      "Number of mini-batches = 179\n",
      "STARTING - ITERATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:107: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 50.7943 sec.\n",
      "iter 0 || Loss: 25.9056 || 08:41:26 AM\n",
      "\n",
      "\n",
      "timer: 1.6040 sec.\n",
      "iter 10 || Loss: 488.8744 || 08:42:54 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 10\n",
      "timer: 3.0976 sec.\n",
      "iter 20 || Loss: 17.8533 || 08:46:08 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 20\n",
      "timer: 2.6946 sec.\n",
      "iter 30 || Loss: 13.3618 || 08:48:30 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 30\n",
      "timer: 2.4810 sec.\n",
      "iter 40 || Loss: 12.4807 || 08:51:34 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 40\n",
      "timer: 2.5022 sec.\n",
      "iter 50 || Loss: 13.3958 || 08:54:00 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 50\n",
      "timer: 3.3990 sec.\n",
      "iter 60 || Loss: 11.4552 || 08:57:17 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 60\n",
      "timer: 2.1050 sec.\n",
      "iter 70 || Loss: 8.6826 || 08:59:36 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 70\n",
      "timer: 2.2085 sec.\n",
      "iter 80 || Loss: 8.8992 || 09:02:58 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 80\n",
      "timer: 2.0925 sec.\n",
      "iter 90 || Loss: 8.8541 || 09:05:21 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 90\n",
      "timer: 1.8959 sec.\n",
      "iter 100 || Loss: 7.7064 || 09:08:32 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 100\n",
      "timer: 2.7000 sec.\n",
      "iter 110 || Loss: 7.7342 || 09:10:55 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 110\n",
      "timer: 2.0997 sec.\n",
      "iter 120 || Loss: 8.2673 || 09:14:06 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 120\n",
      "timer: 3.5002 sec.\n",
      "iter 130 || Loss: 8.4582 || 09:16:22 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 130\n",
      "timer: 2.9126 sec.\n",
      "iter 140 || Loss: 7.6549 || 09:19:34 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 140\n",
      "timer: 2.2985 sec.\n",
      "iter 150 || Loss: 7.6807 || 09:21:48 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 150\n",
      "timer: 1.7087 sec.\n",
      "iter 160 || Loss: 7.2880 || 09:25:01 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 160\n",
      "timer: 2.1997 sec.\n",
      "iter 170 || Loss: 7.6506 || 09:27:13 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 170\n",
      "timer: 2.1983 sec.\n",
      "iter 180 || Loss: 7.8881 || 09:30:14 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 180\n",
      "timer: 2.2945 sec.\n",
      "iter 190 || Loss: 7.6494 || 09:32:31 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 190\n",
      "timer: 2.3959 sec.\n",
      "iter 200 || Loss: 7.9678 || 09:35:44 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 200\n",
      "timer: 1.9878 sec.\n",
      "iter 210 || Loss: 7.7266 || 09:38:03 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 210\n",
      "timer: 2.0944 sec.\n",
      "iter 220 || Loss: 7.6023 || 09:41:19 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 220\n",
      "timer: 1.7969 sec.\n",
      "iter 230 || Loss: 7.4984 || 09:43:35 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 230\n",
      "timer: 3.0935 sec.\n",
      "iter 240 || Loss: 7.7331 || 09:46:52 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 240\n",
      "timer: 1.9089 sec.\n",
      "iter 250 || Loss: 8.0859 || 09:49:05 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 250\n",
      "timer: 3.4112 sec.\n",
      "iter 260 || Loss: 7.4882 || 09:52:11 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 260\n",
      "timer: 3.4023 sec.\n",
      "iter 270 || Loss: 7.8318 || 09:54:50 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 270\n",
      "timer: 2.1987 sec.\n",
      "iter 280 || Loss: 7.9692 || 09:57:47 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 280\n",
      "timer: 2.4959 sec.\n",
      "iter 290 || Loss: 7.1372 || 10:00:11 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 290\n",
      "timer: 1.5041 sec.\n",
      "iter 300 || Loss: 7.8895 || 10:03:25 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 300\n",
      "timer: 2.4158 sec.\n",
      "iter 310 || Loss: 7.5210 || 10:05:51 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 310\n",
      "timer: 1.9019 sec.\n",
      "iter 320 || Loss: 8.1782 || 10:09:01 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 320\n",
      "timer: 1.1059 sec.\n",
      "iter 330 || Loss: 7.6938 || 10:11:20 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 330\n",
      "timer: 1.0212 sec.\n",
      "iter 340 || Loss: 7.4430 || 10:14:32 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 340\n",
      "timer: 1.7816 sec.\n",
      "iter 350 || Loss: 7.6568 || 10:16:52 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 350\n",
      "timer: 2.3949 sec.\n",
      "iter 360 || Loss: 7.6181 || 10:19:44 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 360\n",
      "timer: 2.3032 sec.\n",
      "iter 370 || Loss: 8.3563 || 10:23:03 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 370\n",
      "timer: 2.2950 sec.\n",
      "iter 380 || Loss: 7.8236 || 10:25:25 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 380\n",
      "timer: 1.9166 sec.\n",
      "iter 390 || Loss: 7.8136 || 10:28:34 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 390\n",
      "timer: 3.0929 sec.\n",
      "iter 400 || Loss: 7.8259 || 10:30:55 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 400\n",
      "timer: 2.2023 sec.\n",
      "iter 410 || Loss: 7.6335 || 10:34:15 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 410\n",
      "timer: 1.9943 sec.\n",
      "iter 420 || Loss: 7.5718 || 10:36:32 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 420\n",
      "timer: 1.4168 sec.\n",
      "iter 430 || Loss: 7.6810 || 10:39:40 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 430\n",
      "timer: 2.1028 sec.\n",
      "iter 440 || Loss: 7.1430 || 10:42:01 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 440\n",
      "timer: 1.9884 sec.\n",
      "iter 450 || Loss: 7.8125 || 10:45:07 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 450\n",
      "timer: 1.1023 sec.\n",
      "iter 460 || Loss: 7.2718 || 10:47:28 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 460\n",
      "timer: 1.8913 sec.\n",
      "iter 470 || Loss: 7.0777 || 10:50:40 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 470\n",
      "timer: 3.2926 sec.\n",
      "iter 480 || Loss: 7.3130 || 10:53:10 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 480\n",
      "timer: 1.9982 sec.\n",
      "iter 490 || Loss: 7.3011 || 10:56:23 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 490\n",
      "timer: 3.3032 sec.\n",
      "iter 500 || Loss: 8.1660 || 10:58:57 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 500\n",
      "timer: 1.6037 sec.\n",
      "iter 510 || Loss: 7.6467 || 11:02:00 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 510\n",
      "timer: 1.4024 sec.\n",
      "iter 520 || Loss: 7.8419 || 11:04:35 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 520\n",
      "timer: 1.2247 sec.\n",
      "iter 530 || Loss: 7.8404 || 11:07:35 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 530\n",
      "timer: 2.0140 sec.\n",
      "iter 540 || Loss: 7.6952 || 11:09:45 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 540\n",
      "timer: 2.4063 sec.\n",
      "iter 550 || Loss: 7.7927 || 11:13:05 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 550\n",
      "timer: 1.5974 sec.\n",
      "iter 560 || Loss: 7.9552 || 11:15:26 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 560\n",
      "timer: 1.8998 sec.\n",
      "iter 570 || Loss: 7.9194 || 11:18:42 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 570\n",
      "timer: 2.5998 sec.\n",
      "iter 580 || Loss: 6.9284 || 11:21:03 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 580\n",
      "timer: 1.1096 sec.\n",
      "iter 590 || Loss: 7.1135 || 11:24:17 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 590\n",
      "timer: 1.5016 sec.\n",
      "iter 600 || Loss: 7.6781 || 11:26:37 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 600\n",
      "timer: 2.6145 sec.\n",
      "iter 610 || Loss: 7.7111 || 11:29:52 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 610\n",
      "timer: 0.9139 sec.\n",
      "iter 620 || Loss: 7.9690 || 11:32:14 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 620\n",
      "timer: 3.7082 sec.\n",
      "iter 630 || Loss: 7.7735 || 11:35:26 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 630\n",
      "timer: 1.8032 sec.\n",
      "iter 640 || Loss: 11.2452 || 11:37:45 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 640\n",
      "timer: 3.2008 sec.\n",
      "iter 650 || Loss: 14.3328 || 11:40:53 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 650\n",
      "timer: 2.5019 sec.\n",
      "iter 660 || Loss: 11.6351 || 11:43:24 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 660\n",
      "timer: 1.1104 sec.\n",
      "iter 670 || Loss: 8.8324 || 11:46:34 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 670\n",
      "timer: 2.2022 sec.\n",
      "iter 680 || Loss: 8.0918 || 11:48:57 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 680\n",
      "timer: 1.5989 sec.\n",
      "iter 690 || Loss: 9.5389 || 11:52:02 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 690\n",
      "timer: 2.2037 sec.\n",
      "iter 700 || Loss: 7.8725 || 11:54:19 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 700\n",
      "timer: 3.0002 sec.\n",
      "iter 710 || Loss: 7.3060 || 11:57:08 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 710\n",
      "timer: 2.8044 sec.\n",
      "iter 720 || Loss: 7.5736 || 11:59:52 AM\n",
      "\n",
      "\n",
      "Saving state, iter: 720\n"
     ]
    }
   ],
   "source": [
    "train(device, resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
