{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision as tv\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import pickle as pkl\n",
    "import random\n",
    "import tarfile\n",
    "import collections\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version_info[0] == 2:\n",
    "    import xml.etree.cElementTree as ET\n",
    "else:\n",
    "    import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every\n",
    "        specified step\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    lr = lr * (gamma ** (step))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def xavier(param):\n",
    "    init.xavier_uniform_(param)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def create_vis_plot(_xlabel, _ylabel, _title, _legend):\n",
    "    return viz.line(\n",
    "        X=torch.zeros((1,)).cpu(),\n",
    "        Y=torch.zeros((1, 3)).cpu(),\n",
    "        opts=dict(\n",
    "            xlabel=_xlabel,\n",
    "            ylabel=_ylabel,\n",
    "            title=_title,\n",
    "            legend=_legend\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def update_vis_plot(iteration, loc, conf, window1, window2, update_type,\n",
    "                    epoch_size=1):\n",
    "    viz.line(\n",
    "        X=torch.ones((1, 3)).cpu() * iteration,\n",
    "        Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu() / epoch_size,\n",
    "        win=window1,\n",
    "        update=update_type\n",
    "    )\n",
    "    # initialize epoch plot on first iteration\n",
    "    if iteration == 0:\n",
    "        viz.line(\n",
    "            X=torch.zeros((1, 3)).cpu(),\n",
    "            Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu(),\n",
    "            win=window2,\n",
    "            update=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    cfg = voc\n",
    "    dataset = VOCDetection(root=dataset_root, image_sets=[('2012', 'train')],\n",
    "                            transform=SSDAugmentation(cfg['min_dim'],\n",
    "                             MEANS))\n",
    "    \n",
    "#     visdom = False\n",
    "#     if visdom:\n",
    "#         import visdom\n",
    "#         viz = visdom.Visdom()\n",
    "\n",
    "    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "    net = ssd_net\n",
    "\n",
    "    if cuda:\n",
    "        net = torch.nn.DataParallel(ssd_net)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    if resume:\n",
    "        print('Resuming training, loading {}...'.format(resume))\n",
    "        ssd_net.load_weights(resume)\n",
    "    else:\n",
    "        vgg_weights = torch.load(basenet)\n",
    "        print('Loading base network...')\n",
    "        ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "    if cuda:\n",
    "         net = net.to(device)\n",
    "\n",
    "    if not resume:\n",
    "        print('Initializing weights...')\n",
    "        # initialize newly added layers' weights with xavier method\n",
    "        ssd_net.extras.apply(weights_init)\n",
    "        ssd_net.loc.apply(weights_init)\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr, momentum,weight_decay)\n",
    "    criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                             False, cuda)\n",
    "\n",
    "    net.train()\n",
    "    name = 'train'\n",
    "    \n",
    "    # loss counters\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    epoch = 0\n",
    "    print('Loading the dataset...')\n",
    "\n",
    "    epoch_size = len(dataset) // batch_size\n",
    "    print('Training SSD on: ',name)\n",
    "#    print('Using the specified args:')\n",
    "#     print(args)\n",
    "\n",
    "    step_index = 0\n",
    "\n",
    "#     if visdom:\n",
    "#         vis_title = 'SSD.PyTorch on ' + dataset.name\n",
    "#         vis_legend = ['Loc Loss', 'Conf Loss', 'Total Loss']\n",
    "#         iter_plot = create_vis_plot('Iteration', 'Loss', vis_title, vis_legend)\n",
    "#         epoch_plot = create_vis_plot('Epoch', 'Loss', vis_title, vis_legend)\n",
    "\n",
    "    data_loader = data.DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True,\\\n",
    "                               collate_fn=detection_collate,pin_memory=True)\n",
    "    \n",
    "    print(\"Number of images in the training set = \" + str(len(dataset)))\n",
    "    print(\"Number of images in a mini-batch = \"+str(batch_size))\n",
    "    print(\"Number of mini-batches = \" + str(len(data_loader)))\n",
    "    \n",
    "    \n",
    "     # create batch iterator\n",
    "    batch_iterator = iter(data_loader)\n",
    "    print(\"STARTING - ITERATIONS\")\n",
    "    \n",
    "    # Stats for pickle\n",
    "    l_loss = []\n",
    "    c_loss = []\n",
    "    itr = []\n",
    "    \n",
    "    for iteration in range(start_iter, 10000):\n",
    "        \n",
    "        if visdom and iteration != 0 and (iteration % epoch_size == 0):\n",
    "            update_vis_plot(epoch, loc_loss, conf_loss, epoch_plot, None,\n",
    "                             'append', epoch_size)\n",
    "            # reset epoch loss counters\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "            epoch += 1\n",
    "\n",
    "        if iteration in cfg['lr_steps']:\n",
    "            step_index += 1\n",
    "            adjust_learning_rate(optimizer, gamma, step_index)\n",
    "\n",
    "            \n",
    "        ## load train data\n",
    "        #images, targets = next(batch_iterator)\n",
    "        try:\n",
    "            images, targets = next(batch_iterator)\n",
    "        except StopIteration:\n",
    "            batch_iterator = iter(data_loader)\n",
    "            images, targets = next(batch_iterator)\n",
    "\n",
    "\n",
    "        \n",
    "        if cuda:\n",
    "            images = Variable(images.cuda())\n",
    "            targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            targets = [Variable(ann, volatile=True) for ann in targets]\n",
    "        \n",
    "        # forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_l + loss_c\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.data.item()\n",
    "        conf_loss += loss_c.data.item()\n",
    "\n",
    "        l_loss.append(loss_l.data.item())\n",
    "        c_loss.append(loss_c.data.item())\n",
    "        itr.append(iteration)\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data.item()), end=' ')\n",
    "            currentDT = datetime.datetime.now()\n",
    "            print (currentDT.strftime(\"%H:%M:%S %p\"))\n",
    "            print(\"\\n\")\n",
    "            \n",
    "#         if visdom:\n",
    "#             update_vis_plot(iteration, loss_l.data.item(), loss_c.data.item(),\n",
    "#                             iter_plot, epoch_plot, 'append')\n",
    "\n",
    "        \n",
    "        if iteration != 0 and iteration % 10 == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            iter_name = math.ceil(iteration/100)*100\n",
    "            torch.save(ssd_net.state_dict(), 'weights/ssd300_COCO_' +repr(iter_name) + '.pth')\n",
    "            with open('stats_SGD.pkl','wb') as f:\n",
    "                pkl.dump([l_loss, c_loss, itr], f)\n",
    "                \n",
    "\n",
    "    torch.save(ssd_net.state_dict(),\n",
    "               save_folder + data_set + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 'VOC'\n",
    "dataset_root = '//datasets/ee285f-public/PascalVOC2012/'\n",
    "basenet = 'weights/vgg16_reducedfc.pth'\n",
    "batch_size = 32\n",
    "resume = None\n",
    "start_iter = 0\n",
    "num_workers = 4\n",
    "cuda = True\n",
    "learning_rate = lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.1\n",
    "visdom = False\n",
    "save_folder = 'trained_weights/'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "\n",
    "# resume = 'ssd300_mAP_77.43_v2.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base network...\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on:  train\n",
      "Number of images in the training set = 5717\n",
      "Number of images in a mini-batch = 32\n",
      "Number of mini-batches = 179\n",
      "STARTING - ITERATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:107: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timer: 5.1993 sec.\n",
      "iter 0 || Loss: 26.4353 || 20:02:55 PM\n",
      "\n",
      "\n",
      "timer: 1.4019 sec.\n",
      "iter 10 || Loss: 16.4076 || 20:05:11 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 10\n",
      "timer: 1.9969 sec.\n",
      "iter 20 || Loss: 15.7623 || 20:07:51 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 20\n",
      "timer: 2.0943 sec.\n",
      "iter 30 || Loss: 13.5455 || 20:10:21 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 30\n",
      "timer: 1.9003 sec.\n",
      "iter 40 || Loss: 14.4571 || 20:13:05 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 40\n",
      "timer: 1.7958 sec.\n",
      "iter 50 || Loss: 11.5304 || 20:15:28 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 50\n",
      "timer: 1.5102 sec.\n",
      "iter 60 || Loss: 10.6264 || 20:18:31 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 60\n",
      "timer: 1.6024 sec.\n",
      "iter 70 || Loss: 10.9013 || 20:20:48 PM\n",
      "\n",
      "\n",
      "Saving state, iter: 70\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('stats.pkl','rb') as f:\n",
    "#     l_loss, c_loss, itr = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
